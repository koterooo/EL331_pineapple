# -*- coding: utf-8 -*-
"""
This code was generated by ChatGPT, an AI language model by OpenAI.

Features:
  • Exact-match token search
  • Lemma-based search (new)
  • POS / Entity search
  • .txt corpus upload (UTF-8 / Shift_JIS)
  • Loading indicator on the front-end

Algorithm overview:
  - Implements a KWIC (Key Word In Context) Web App using Flask.
  - Uses spaCy for linguistic processing (tokenization, lemmatization, POS tagging, NER).
  - Supports token/lemma/POS/entity search and KWIC extraction with context window.
  - Provides sorting by sequential order, next-token frequency, or next-POS frequency.
  - Displays most frequent next-token patterns.
"""

from flask import Flask, render_template, request
import spacy
from collections import Counter
import os

app = Flask(__name__)
nlp = spacy.load("en_core_web_sm")

IGNORED_TOKENS = {"(", ")", ",", ".", ":", ";"}

POS_TAGS = [
    "ADJ", "ADP", "ADV", "AUX", "CCONJ", "DET", "INTJ", "NOUN", "NUM", "PART",
    "PRON", "PROPN", "PUNCT", "SCONJ", "SYM", "VERB", "X"
]
ENT_LABELS = [
    "PERSON", "NORP", "FAC", "ORG", "GPE", "LOC", "PRODUCT", "EVENT", "WORK_OF_ART",
    "LAW", "LANGUAGE", "DATE", "TIME", "PERCENT", "MONEY", "QUANTITY",
    "ORDINAL", "CARDINAL"
]

# --------------------------------------------------------------------------- #
# Main view – handles both GET (initial) and POST (search) requests           #
# --------------------------------------------------------------------------- #
@app.route("/", methods=["GET", "POST"])
def index():
    """
    Main view function for KWIC Web App.

    Algorithm:
      - Accepts corpus input (via textarea or .txt file upload).
      - Accepts search parameters: target string, search type, context window size, sort mode.
      - Processes text with spaCy NLP pipeline.
      - Searches for matches based on search type:
          * Token: exact token match (case-insensitive)
          * Lemma: exact lemma match (case-insensitive)
          * POS: matches POS tag
          * Entity: matches NER label
      - For each match, extracts left/right context (window), keyword, and next-token info.
      - Counts next-token patterns for pattern statistics.
      - Sorts results based on user-selected mode.
      - Returns results and pattern statistics to HTML template.
    """
    result, patterns = [], []

    if request.method == "POST":
        # --- 1. Corpus text input (from textarea or file upload) --- #
        text = request.form.get("text", "").strip()
        if not text and "file" in request.files:
            file = request.files["file"]
            if file and file.filename.endswith(".txt"):
                # Try both UTF-8 and Shift_JIS encodings
                for enc in ("utf-8", "shift_jis"):
                    try:
                        file.seek(0)
                        text = file.read().decode(enc)
                        break
                    except UnicodeDecodeError:
                        continue
                if not text:
                    text = "Error: Unable to decode file. Use UTF-8 or Shift_JIS."

        # --- 2. Read search parameters from form --- #
        target   = request.form.get("target", "").strip()
        s_type   = request.form.get("search_type", "token")   # token / lemma / pos / entity
        window   = int(request.form.get("window", 5))
        s_mode   = request.form.get("sort_mode", "sequential")

        # --- 3. NLP processing using spaCy --- #
        doc = nlp(text.replace("\n", " "))
        matches = []  # list of (match_start_index, match_span_length)

        # --- 3-A. Token (exact match) search --- #
        if s_type == "token":
            tgt_doc   = nlp(target)
            tgt_txts  = [t.text.lower() for t in tgt_doc]
            span_len  = len(tgt_txts)

            for i in range(len(doc) - span_len + 1):
                ok = True
                for j in range(span_len):
                    if doc[i + j].text.lower() != tgt_txts[j]:
                        ok = False
                        break
                if ok and doc[i].text not in IGNORED_TOKENS:
                    matches.append((i, span_len))

        # --- 3-B. Lemma search (match on lemmatized form) --- #
        elif s_type == "lemma":
            tgt_doc   = nlp(target)
            tgt_lems  = [t.lemma_.lower() for t in tgt_doc]
            span_len  = len(tgt_lems)

            for i in range(len(doc) - span_len + 1):
                ok = True
                for j in range(span_len):
                    if doc[i + j].lemma_.lower() != tgt_lems[j]:
                        ok = False
                        break
                if ok and doc[i].text not in IGNORED_TOKENS:
                    matches.append((i, span_len))

        # --- 3-C. POS tag search (exact POS match) --- #
        elif s_type == "pos":
            for i, tok in enumerate(doc):
                if tok.pos_ == target and tok.text not in IGNORED_TOKENS:
                    matches.append((i, 1))

        # --- 3-D. Entity label (NER) search --- #
        elif s_type == "entity":
            for ent in doc.ents:
                if ent.label_ == target.upper() and doc[ent.start].text not in IGNORED_TOKENS:
                    matches.append((ent.start, ent.end - ent.start))

        # --- 4. Build KWIC lines and count next-token patterns --- #
        pattern_counter, output = Counter(), []
        for idx, span_len in matches:
            # Find the sentence containing the matched token(s)
            sent = next(s for s in doc.sents if s.start <= idx < s.end)
            loc  = idx - sent.start
            toks = [t.text for t in sent]

            # Extract context windows
            left  = toks[max(0, loc - window): loc]
            mid   = toks[loc: loc + span_len]
            right = toks[loc + span_len: loc + span_len + window]

            # Gather next-token information for pattern statistics
            next_tok = sent[loc + span_len] if (loc + span_len) < len(sent) else None
            if next_tok:
                pattern_counter[(next_tok.text, next_tok.pos_, next_tok.ent_type_ or "")] += 1

            # Append KWIC row
            output.append({
                "left":  " ".join(left),
                "mid":   " ".join(mid),
                "right": " ".join(right),
                "next_word": next_tok.text.lower() if next_tok else "",
                "next_pos":  next_tok.pos_       if next_tok else ""
            })

        # --- 5. Sort results as specified --- #
        if s_mode == "token_freq":
            # Sort by most frequent next-token (descending)
            freq = Counter(o["next_word"] for o in output if o["next_word"])
            output.sort(key=lambda x: (-freq.get(x["next_word"], 0), x["left"], x["right"]))
        elif s_mode == "pos_freq":
            # Sort by most frequent next-POS (descending)
            pfreq = Counter(o["next_pos"] for o in output if o["next_pos"])
            output.sort(key=lambda x: (-pfreq.get(x["next_pos"], 0), x["left"], x["right"]))

        result   = output
        patterns = pattern_counter.most_common(10)

    # --- 6. Render template with results and statistics --- #
    return render_template(
        "index.html",
        result=result,
        patterns=patterns,
        pos_tags=POS_TAGS,
        ent_labels=ENT_LABELS
    )

# --------------------------------------------------------------------------- #
if __name__ == "__main__":
    # Run Flask development server
    app.run(debug=True, host="0.0.0.0", port=int(os.environ.get("PORT", 5000)))
